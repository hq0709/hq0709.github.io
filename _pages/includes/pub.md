
# üìù Publications 


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/eye.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Eye-gaze Guided Multi-modal Alignment forMedical Representation Learning](https://arxiv.org/pdf/2403.12416) \\
Chong Ma, **Hanqi Jiang**, Wenting Chen, Yiwei Li, Zihao Wu, Xiaowei Yu, Zhengliang Liu, Lei Guo, Dajiang Zhu, Tuo Zhang, Dinggang Shen, Tianming Liu, Xiang Li

<!-- [**Project**](https://speechresearch.github.io/fastspeech/)  -->

- We propose EGMA, a novel framework for medical multi-modal alignment, marking the first attempt to integrate eye-gaze data into vision-language pre-training.
- EGMA outperforms existing state-of-the-art medical multi-modal pre-training methods, and realizes notable enhancements in image classification and image-text retrieval tasks.
- EGMA demonstrates that even a small amount of eye-gaze data can effectively assist in multi-modal pre-training and improve the feature representation ability of the model.
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024 Workshop</div><img src='images/hybrid.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Advancing Medical Radiograph Representation Learning: A Hybrid Pre-training Paradigm with Multilevel Semantic Granularity](https://arxiv.org/abs/2410.00448) \\
**Hanqi Jiang**, Xixuan Hao, Yuzhou Huang, Chong Ma, Jiaxun Zhang, Yi Pan, Ruimao Zhang

<!-- [**Project**](https://speechresearch.github.io/fastspeech2/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:LkGwnXOMwfcC'></span></strong> -->
  - we present a medical vision-language pre-training (Med-VLP) framework that incorporates multi-modal contrastive alignment and parallel generative streams with multi-level semantic hierarchies. To accomplish this goal, we effectively leverage the characteristics of medical data. By optimizing elaborate training objectives, our HybridMED is capable of efficiently executing a variety of downstream tasks, including cross-modal, uni-modal, and multi-modal types. Extensive experimental results demonstrate that our HybridMED can deliver highly satisfactory performance across a wide array of downstream tasks, thereby validating the model's superiority.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Under Review</div><img src='images/pipeline_EchoPulse.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[ECHOPluse: ECG Controlled Echocardiograms Video Generation](https://arxiv.org/pdf/2410.03143) \\
Yiwei Li, Sekeun Kim, Zihao Wu, **Hanqi Jiang**, Yi Pan, Pengfei Jin, Sifan Song, Yucheng Shi, Xiaowei Yu, Tianze Yang, Tianming Liu, Quanzheng Li, Xiang Li

<!-- [**Project**](https://speechresearch.github.io/fastspeech2/) <strong><span class='show_paper_citations' data='4FA6C0AAAAAJ:LkGwnXOMwfcC'></span></strong> -->
  - We propose ECHOPluse, an ECG-conditioned ECHO video generation model. ECHOPluse introduces two key advancements: (1) it accelerates ECHO video generation by leveraging VQ-VAE tokenization and masked visual token modeling for fast decoding, and (2) it conditions on readily accessible ECG signals, which are highly coherent with ECHO videos, bypassing complex conditional prompts. To the best of our knowledge, this is the first work to use time-series prompts like ECG signals for ECHO video generation. ECHOPluse not only enables controllable synthetic ECHO data generation but also provides updated cardiac function information for disease monitoring and prediction beyond ECG alone. Evaluations on three public and private datasets demonstrate state-of-the-art performance in ECHO video generation across both qualitative and quantitative measures. Additionally, ECHOPluse can be easily generalized to other modality generation tasks, such as cardiac MRI, fMRI, and 3D CT generation. 
  - [Demo](https://github.com/levyisthebest/ECHOPulse_Prelease)
</div>
</div>

- ``ICIC 2024`` <span style="color:red">(Oral)</span> [Depth-NeuS: Neural Implicit Surfaces Learning for Multi-view Reconstruction Based on Depth Information Optimization](https://arxiv.org/pdf/2303.17088) **Hanqi Jiang**, Cheng Zeng, Runnan Chen, Shuai Liang, Yinhe Han, Yichao Gao, Conglin Wang
- ``MICCAI 2024 Workshop`` [Radiology-GPT: A Large Language Model for Radiology](https://arxiv.org/pdf/2306.08666) Zhengliang Liu, Yiwei Li, Peng Shu, Aoxiao Zhong, **Hanqi Jiang**, Yi Pan, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Quanzheng Li, Tianming Liu, and Xiang Li
- ``Under Review`` [Large Language Models for Robotics: Opportunities, Challenges, and Perspectives](https://arxiv.org/pdf/2401.04334) Jiaqi Wang, Zihao Wu, Yiwei Li, **Hanqi Jiang**, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Huaqin Zhao, Zhengliang Liu, Haixing Dai, Lin Zhao, Bao Ge, Xiang Li, Tianming Liu‚Ä†, and Shu Zhang
- ``Under Review`` [Artificial General Intelligence for Medical Imaging Analysis](https://arxiv.org/pdf/2306.05480) Xiang Li, Lin Zhao, Lu Zhang, Zihao Wu, Zhengliang Liu, **Hanqi Jiang**, Chao Cao, Shaochen Xu, Yiwei Li, Haixing Dai, Yixuan Yuan, Jun Liu, Gang Li, Dajiang Zhu, Pingkun Yan, Quanzheng Li, Wei Liu, Tianming Liu, Dinggang Shen
- ``Under Review`` [A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks](https://arxiv.org/pdf/2408.01319) Jiaqi Wang, **Hanqi Jiang**, Yiheng Liu, Chong Ma, Xu Zhang, Yi Pan, Mengyuan Liu, Peiran Gu, Sichen Xia, Wenjun Li, Yutong Zhang, Zihao Wu, Zhengliang Liu, Tianyang Zhong, Bao Ge, Tuo Zhang, Ning Qiang, Xintao Hu, Xi Jiang, Xin Zhang, Wei Zhang, Dinggang Shen, Tianming Liu, Shu Zhang
- ``Under Review`` [Argus: Leveraging Multi-ViewImages for Improved 3D SceneUnderstanding with LargeLanguage Models]() Yifan Xu, Chao Zhang, **Hanqi Jiang**, Xiaoyan Wang, Ruifei Ma, Yiwei Li, Zihao Wu, Zeju Li, Xiangde Liu
- ``Under Review`` [GeoDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor Scene Generation]() Zhifei Yang, Keyang Lu, Chao Zhang, Jiaxing Qi, **Hanqi Jiang**, Ruifei Ma, Shenglin Yin, Yifan Xu, Mingzhe Xing, Zhen Xiao, Jieyi Long, Xiangde Liu, Guangyao Zhai
- ``Under Review`` [BrainSF: A Foundation Model for Whole Brain Functional Signals Forecasting]() Li Yang, **Hanqi Jiang**, Xintao Hu, Tianming Liu, Tuo Zhang
- ``Under Review`` [EG-SpikeFormer: Eye-Gaze Guided Transformer on Spiking Neural Networks for Medical Image Analysis](https://arxiv.org/pdf/2410.09674) Yi Pan, **Hanqi Jiang**, Junhao Chen, Yiwei Li, Huaqin Zhao, Yifan Zhou, Peng Shu, Zihao Wu, Zhengliang Liu, Dajiang Zhu, Xiang Li, Yohannes Abate, Tianming Liu
<!-- - ``Under Review`` [ECHOPluse: ECG Controlled Echocardiograms Video Generation](https://arxiv.org/pdf/2410.03143) Yiwei Li, Sekeun Kim, Zihao Wu, **Hanqi Jiang**, Yi Pan, Pengfei Jin, Sifan Song, Yucheng Shi, Xiaowei Yu, Tianze Yang, Tianming Liu, Quanzheng Li, Xiang Li -->